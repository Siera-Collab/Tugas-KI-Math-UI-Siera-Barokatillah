{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNesRh00pTg19SIOJpegbfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siera-Collab/Tugas-KI-Math-UI-Siera-Barokatillah/blob/main/TaskWeek7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tugas Minggu ke-7**"
      ],
      "metadata": {
        "id": "Rllc0_pOsePN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Siera Barokatillah"
      ],
      "metadata": {
        "id": "YuulSHypscPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "NPM : 2006568714"
      ],
      "metadata": {
        "id": "SdIOjXlYr1EH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Souce of model : https://huggingface.co/models?search=sentiment%20analysis"
      ],
      "metadata": {
        "id": "dbKUZPOPsxH9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyuW4WtaGboz",
        "outputId": "752f1050-9209-466c-eaca-c0aab5d96c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.2160816788673401,\n",
              "  'token': 35698,\n",
              "  'token_str': ' waitress',\n",
              "  'sequence': 'The woman worked as a waitress.'},\n",
              " {'score': 0.07409851253032684,\n",
              "  'token': 38233,\n",
              "  'token_str': ' waiter',\n",
              "  'sequence': 'The woman worked as a waiter.'},\n",
              " {'score': 0.06863326579332352,\n",
              "  'token': 33080,\n",
              "  'token_str': ' bartender',\n",
              "  'sequence': 'The woman worked as a bartender.'},\n",
              " {'score': 0.05877527594566345,\n",
              "  'token': 9008,\n",
              "  'token_str': ' nurse',\n",
              "  'sequence': 'The woman worked as a nurse.'},\n",
              " {'score': 0.05221009626984596,\n",
              "  'token': 29754,\n",
              "  'token_str': ' maid',\n",
              "  'sequence': 'The woman worked as a maid.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='distilroberta-base')\n",
        "unmasker(\"The man worked as a <mask>.\")\n",
        "unmasker(\"The woman worked as a <mask>.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = model(encoded_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3xfN7ahI4Pl",
        "outputId": "6acac212-a255-4171-a21c-d7ff4385a0f7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model dan tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\")\n",
        "\n",
        "# Contoh kalimat untuk prediksi\n",
        "inputs = tokenizer(\"Saya senang belajar pemrograman.\", return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Mendapatkan hasil prediksi\n",
        "predicted_class_id = logits.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "print(f\"Sentimen: {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejfTBd46kWfl",
        "outputId": "154b0bd6-dc59-4c08-b190-b39436bfa205"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentimen: POSITIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.id2label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L60hfwClnGX",
        "outputId": "740754b7-5277-49f4-8daa-c185a2d62a50"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'POSITIVE', 1: 'NEUTRAL', 2: 'NEGATIVE'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the modelâ€™s accuracy using a synthetic dataset**\n"
      ],
      "metadata": {
        "id": "h85jxaQXtcYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Dataset sintetis dengan label yang sesuai dengan model\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"Saya sangat senang belajar!\",       # POSITIVE\n",
        "        \"Ini adalah hari yang buruk.\",       # NEGATIVE\n",
        "        \"Saya tidak peduli tentang hal ini.\" # NEUTRAL\n",
        "    ],\n",
        "    \"label\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n",
        "}\n",
        "\n",
        "# Konversi ke DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Prediksi setiap teks dalam dataset sintetis\n",
        "predictions = []\n",
        "for text in df[\"text\"]:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    sentiment = model.config.id2label[predicted_class_id]\n",
        "    predictions.append(sentiment)\n",
        "\n",
        "# Tambahkan prediksi ke DataFrame\n",
        "df[\"predicted_label\"] = predictions\n",
        "\n",
        "# Hitung akurasi\n",
        "accuracy = (df[\"label\"] == df[\"predicted_label\"]).mean()\n",
        "print(f\"Akurasinya adalah: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ohtfk99mC3j",
        "outputId": "c65cada5-789a-4f89-f153-bcb774418827"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasinya adalah: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement attention transformer on that model**"
      ],
      "metadata": {
        "id": "unbjflJ6th5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load ulang model dengan output layer perhatian diaktifkan\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\", output_attentions=True)\n",
        "\n",
        "# Contoh kalimat\n",
        "inputs = tokenizer(\"Saya senang belajar pemrograman.\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Ambil layer perhatian\n",
        "attentions = outputs.attentions\n",
        "print(f\"Jumlah layer perhatian: {len(attentions)}\")\n",
        "print(f\"Bentuk data layer perhatian: {attentions[0].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO6k0El_mIP_",
        "outputId": "77f0f1de-a516-45f7-bca2-ad950cd29cf0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah layer perhatian: 12\n",
            "Bentuk data layer perhatian: torch.Size([1, 12, 7, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the model accuracy after using attention transformer using the same synthetic dataset and compare**"
      ],
      "metadata": {
        "id": "mLFa4iUVtrNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load ulang model dengan output layer perhatian diaktifkan\n",
        "model_with_attention = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\",\n",
        "    output_attentions=True\n",
        ")\n",
        "\n",
        "# Prediksi setiap teks dalam dataset sintetis dengan model baru\n",
        "new_predictions = []\n",
        "for text in df[\"text\"]:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model_with_attention(**inputs)\n",
        "        logits = outputs.logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    sentiment = model_with_attention.config.id2label[predicted_class_id]\n",
        "    new_predictions.append(sentiment)\n",
        "\n",
        "# Tambahkan prediksi baru ke DataFrame\n",
        "df[\"new_predicted_label\"] = new_predictions\n",
        "\n",
        "# Hitung akurasi baru\n",
        "new_accuracy = (df[\"label\"] == df[\"new_predicted_label\"]).mean()\n",
        "print(f\"Akurasinya setelah analisis perhatian adalah: {new_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72DNPMlNroJP",
        "outputId": "dd5d3d06-efe8-4bdf-c8d7-679f1d9413f4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasinya setelah analisis perhatian adalah: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Catatan: Mengaktifkan lapisan perhatian (output_attentions=True) sendiri tidak akan meningkatkan akurasi kecuali model tersebut dilatih kembali (fine-tuning) dengan memanfaatkan lapisan perhatian."
      ],
      "metadata": {
        "id": "FGkpKNR7tt3a"
      }
    }
  ]
}